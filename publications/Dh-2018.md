# Making Public Records Legible 

## Abstract
 In the following paper we describe an open-source transcription engine and platform for crowdsourced editing of legislative documents. We situate this work in the context of research infrastructures that support public humanities research. 

## Public Records Archives

"It will be of little avail to the people that the laws are made by men of their own choice if the laws be so voluminous that they cannot be read, or so incoherent that they cannot be understood." James Madison, The Federalist 62.

Government transparency is a democratic ideal often championed by elected officials in the name of accountability. Recent initiatives focused on the publication of open data, and the development of civic technologies are illustrative examples of how these ideals are being mapped onto a contemporary information communication technology (ICT) landscape. However, the results of these well-intentioned initiatives (e.g. open data portals, legislative document repositories, etc) often obscure as much as they reveal (Janssen et al., 2012) . That is, these initiatives often produce archives of *public* records that are difficult for the general *public* to search, access, and use. The practice of preserving and making accessible the records of a government - in a form which the polity is equipped to meaningfully use - is a traditional dilemma for archivists and historians building public humanities infrastructures. However, we argue that a background in which public records are rapidly generated, and of increasingly complex forms requires renewed attention if both parities are to reliably preserve and make accessible these cultural artifacts. 

In our current research, we have developed a web-based application to transform media recordings (audio and video) into text transcripts, an indexing service for resolving queries against these transcripts, and an interactive web-interface for crowdsourced editing of the transcripts. The open-sourced transcription engine we have developed is able to be adapted to both web-based scraping of websites that link to multimedia archives, as well as an API plug-in for one of the most commonly used content-management systems for legislative record-keeping. The following sections describe a use case, the software dependencies, and the system architecture of this application. 

## Context
City councils in the USA often record legislative sessions as digital video, and then post these files to the web for public access. The video recordings are extremely valuable in that that they exhaustively capture the proceedings of a public legislative session (e.g. Who speaks, when, what is said, in what context, etc.). However, without a text-based transcript the content of these recordings is of limited value. For example, imagine a citizen wanted to better understand the current legislative debate related to zoning regulations in her rapidly gentrifying city. Currently, this would require reading meeting minutes of a legislative session (to understand what topics were addressed in which sessions), and then watching a number of videos in which the subject was discussed. Or, imagine a public administration scholar wanting to understand the rhetorical frames used by incumbent City Council representatives over the last decade. To undertake this research she would need to both search for relevant videos and manually transcribe the speech acts of each individual City Council member - making such a study incredibly labor intensive, and unlikely to be reliably completed. So, while the video records are exhaustive they are of limited value for search, retrieval, and browsing for the general public. This brings into question how transparent governments are when they produce records which are difficult to reliably access and use. 
  
## Transcription Engine
We develop a platform based on a number of open-source tools to solve this problem. 

- Video Files: We obtain videos from city council legislative session by either scraping a website where links are posted, or by querying an API to a content management system in use by city council's throughout the USA (Legistar). Our application makes either the scraping or the API call easily modifiable to a particular city, or particular API.  
- Video to Audio: We then transform each video to an audio stream using the `FFMpeg` framework (Bellard \& Niedermayer, 2012). 
- Audio to Text: Using the Python package `SpeechRecognition` (Zhang, 2017) we separate each audio file into discrete seventeen second chunks to pass through the Google Speech Recognition API - a free service for producing audio-text transcripts. Our platform recombines each seventeen second transcribed section into a complete transcript. 
- Corpus Indexing: For all transcribed legislative sessions, we then implement a ’fuzzy search’ utility by using the text retrieval approach ’Term Frequency - Inverse Document Frequency’ (TF-IDF). We construct a tree of information about each transcribed document in the corpus, and then index this information for the entire corpus. After creating the tree, we created a simple searching method that takes a search phrase (provided by the user of our application) and read the tree to find the most relevant nodes (documents) by their TFIDF score.  
- GUI: The platform also includes an interface for users to search for, discover, and edit transcripts that may have errors (due to the automated transcription).

## Implementation
We have developed a single implementation of this platform for the City of Seattle Washington available at https://councildataproject.github.io/seattle/. Given another city's data in our standard form, the GUI would work with near-zero changes. This implementation is a proof of concept for public testing. In future work, we will describe evaluation with users, and the development of additional features for transcript editing, anchoring video to transcript sections, download of transcripts, and automatically recognizing turn-taking of speakers. 

## Conclusion 
Public scholarship - the “making knowledge ‘about, for, and with’ diverse publics and communities” (Woodward, 2009) - has been a common thread amongst humanists focused on the development of public records archives. The project we have described is aimed at transforming the documentary practices of state and local governments from multi-media recordings to discoverable and searchable textual archives. Through this process we argue that our application makes public records legible for public consumption, improves the transparency of local governments, and enables public scholars to better use text-based corpora for answering research questions about the 

# Works Cited 
@book{hamilton1966federalist,
  title={The Federalist papers: A collection of essays written in support of the constitution of the United States: From the original text of Alexander Hamilton, James Madison, John Jay},
  author={Hamilton, Alexander and Madison, James and Jay, John and Fairfield, Roy P},
  year={1966},
  publisher={Johns Hopkins University Press}
}

@article{woodward2009future,
  title={The future of the humanities-in the present \& in public},
  author={Woodward, Kathleen},
  journal={Daedalus},
  volume={138},
  number={1},
  pages={110--123},
  year={2009},
  publisher={MIT Press}
}

@article{janssen2012benefits,
  title={Benefits, adoption barriers and myths of open data and open government},
  author={Janssen, Marijn and Charalabidis, Yannis and Zuiderwijk, Anneke},
  journal={Information systems management},
  volume={29},
  number={4},
  pages={258--268},
  year={2012},
  publisher={Taylor \& Francis}
}

@article{bellard2012ffmpeg,
  title={FFmpeg},
  author={Bellard, Fabrice and Niedermayer, M and others},
  journal={Available from: http://ffmpeg.org},
  year={2012}
}

@article{ zhang2017,
  title={Speech Recognition},
  author={Zhang, Anthony},
  journal={Version 3.7 Available from:https://github.com/Uberi/speech_recognition#readme},
  }
